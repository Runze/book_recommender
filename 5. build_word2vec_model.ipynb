{
 "metadata": {
  "name": "",
  "signature": "sha256:5db20f2327254150cbe64cc7890080c21ce95f6c118e74b5ec7e7c23ca106fc3"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import pandas as pd\n",
      "from pandas import DataFrame\n",
      "import re\n",
      "import nltk\n",
      "import logging\n",
      "from gensim.models import word2vec\n",
      "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
      "%cd /Users/Runze/Google Drive/book_recommender"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "/Users/Runze/Google Drive/book_recommender\n"
       ]
      }
     ],
     "prompt_number": 1
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "nyt_npr_gr = pd.read_csv('nyt_npr_gr.csv')\n",
      "desc = nyt_npr_gr['desc']"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 2
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# clean descriptions and parse them into sentences made up of word lists\n",
      "def sent_to_words(sent):\n",
      "    sent = re.sub(r'[^a-z]', ' ', sent.lower())\n",
      "    words = sent.split()\n",
      "    return words\n",
      "\n",
      "sent_tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
      "def desc_to_sents(desc):\n",
      "    desc = re.sub(r'\\xc2\\xa0', ' ', desc)\n",
      "    desc = re.sub(r'\\xc2\\xad', ' ', desc)\n",
      "    desc = re.sub(r'\\xc2\\xb7', ' ', desc)\n",
      "    \n",
      "    sents = sent_tokenizer.tokenize(desc.strip())\n",
      "    sents = [sent_to_words(sent) for sent in sents]\n",
      "    return sents\n",
      "\n",
      "desc = [desc_to_sents(d) for d in desc]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 3
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "desc = sum(desc, [])\n",
      "len(desc)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 4,
       "text": [
        "22781"
       ]
      }
     ],
     "prompt_number": 4
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# train a word vector model\n",
      "word2vec_model = word2vec.Word2Vec(desc, workers = 4, min_count = 10, size = 300, window = 50, sample = .001, seed = 2015)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "2015-01-05 22:03:38,070 : INFO : collecting all words and their counts\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "2015-01-05 22:03:38,071 : INFO : PROGRESS: at sentence #0, processed 0 words and 0 word types\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "2015-01-05 22:03:38,211 : INFO : PROGRESS: at sentence #10000, processed 228908 words and 20948 word types\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "2015-01-05 22:03:38,306 : INFO : PROGRESS: at sentence #20000, processed 443488 words and 30405 word types\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "2015-01-05 22:03:38,338 : INFO : collected 32717 word types from a corpus of 504695 words and 22781 sentences\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "2015-01-05 22:03:38,354 : INFO : total 5048 word types after removing those with count<10\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "2015-01-05 22:03:38,354 : INFO : constructing a huffman tree from 5048 words\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "2015-01-05 22:03:38,517 : INFO : built huffman tree with maximum node depth 15\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "2015-01-05 22:03:38,520 : INFO : frequent-word downsampling, threshold 0.001; progress tallies will be approximate\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "2015-01-05 22:03:38,532 : INFO : resetting layer weights\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "2015-01-05 22:03:38,628 : INFO : training model with 4 workers on 5048 vocabulary and 300 features, using 'skipgram'=1 'hierarchical softmax'=1 'subsample'=0.001 and 'negative sampling'=0\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "2015-01-05 22:03:39,784 : INFO : PROGRESS: at 12.41% words, alpha 0.02240, 33974 words/s\n",
        "2015-01-05 22:03:40,821 : INFO : PROGRESS: at 22.92% words, alpha 0.01994, 33064 words/s\n",
        "2015-01-05 22:03:41,822 : INFO : PROGRESS: at 36.26% words, alpha 0.01646, 35896 words/s\n",
        "2015-01-05 22:03:42,828 : INFO : PROGRESS: at 48.97% words, alpha 0.01316, 36863 words/s\n",
        "2015-01-05 22:03:43,847 : INFO : PROGRESS: at 64.23% words, alpha 0.00935, 38912 words/s\n",
        "2015-01-05 22:03:44,871 : INFO : PROGRESS: at 79.46% words, alpha 0.00553, 40243 words/s\n",
        "2015-01-05 22:03:45,903 : INFO : PROGRESS: at 91.14% words, alpha 0.00267, 39613 words/s\n",
        "2015-01-05 22:03:46,357 : INFO : reached the end of input; waiting to finish 8 outstanding jobs\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "2015-01-05 22:03:46,670 : INFO : training on 316478 words took 8.0s, 39359 words/s\n"
       ]
      }
     ],
     "prompt_number": 5
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# test the model with a couple examples\n",
      "word2vec_model.most_similar('harry')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "2015-01-05 22:03:46,674 : INFO : precomputing L2-norms of word weight vectors\n"
       ]
      },
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 6,
       "text": [
        "[('potter', 0.9447231292724609),\n",
        " ('halfblood', 0.8756675124168396),\n",
        " ('prisoner', 0.7735387086868286),\n",
        " ('hogwarts', 0.745739221572876),\n",
        " ('next', 0.7412007451057434),\n",
        " ('treasure', 0.7280822396278381),\n",
        " ('dead', 0.7135874032974243),\n",
        " ('strange', 0.7073667645454407),\n",
        " ('evil', 0.6917257308959961),\n",
        " ('fire', 0.6913785934448242)]"
       ]
      }
     ],
     "prompt_number": 6
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "word2vec_model.most_similar(positive = ['king', 'woman'], negative = ['man'])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 7,
       "text": [
        "[('princess', 0.6813679933547974),\n",
        " ('prince', 0.6573864817619324),\n",
        " ('escaped', 0.6443726420402527),\n",
        " ('named', 0.6337329149246216),\n",
        " ('queen', 0.633719265460968),\n",
        " ('valley', 0.6204538345336914),\n",
        " ('beautiful', 0.6196812391281128),\n",
        " ('sea', 0.6183556318283081),\n",
        " ('three', 0.6176857352256775),\n",
        " ('academy', 0.6124534606933594)]"
       ]
      }
     ],
     "prompt_number": 7
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# save the model\n",
      "word2vec_model.init_sims(replace = True)\n",
      "word2vec_model.save('word2vec_model')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "2015-01-05 22:03:46,715 : INFO : precomputing L2-norms of word weight vectors\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "2015-01-05 22:03:46,775 : INFO : saving Word2Vec object under word2vec_model, separately None\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "2015-01-05 22:03:46,777 : INFO : not storing attribute syn0norm\n"
       ]
      }
     ],
     "prompt_number": 8
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# combine the word vector matrix with the chosen vocabulary\n",
      "word2vec_matrix = DataFrame(word2vec_model.syn0, index = word2vec_model.index2word)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 9
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# remove stop words from the matrix\n",
      "stop_words = nltk.corpus.stopwords.words('english')\n",
      "stop_words = set(stop_words) & set(word2vec_model.index2word)\n",
      "print stop_words\n",
      "\n",
      "word2vec_matrix = word2vec_matrix.drop(stop_words)\n",
      "print '\\n', word2vec_model.syn0.shape, word2vec_matrix.shape"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "set([u'all', u'just', u'being', u'over', u'both', u'through', u'its', u'before', u'with', u'had', u'should', u'to', u'only', u'under', u'has', u'do', u'them', u'his', u'very', u'they', u'not', u'during', u'now', u'him', u'nor', u'did', u'this', u't', u'each', u'where', u'because', u'doing', u'some', u'are', u'our', u'ourselves', u'out', u'what', u'for', u'below', u'does', u'above', u'between', u'she', u'be', u'we', u'after', u'here', u'by', u'on', u'about', u'of', u'against', u's', u'or', u'own', u'into', u'yourself', u'down', u'your', u'from', u'her', u'whom', u'there', u'been', u'few', u'too', u'themselves', u'was', u'until', u'more', u'himself', u'that', u'but', u'off', u'herself', u'than', u'those', u'he', u'me', u'myself', u'these', u'up', u'will', u'while', u'can', u'were', u'my', u'and', u'then', u'is', u'in', u'am', u'it', u'an', u'as', u'itself', u'at', u'have', u'further', u'their', u'if', u'again', u'no', u'when', u'same', u'any', u'how', u'other', u'which', u'you', u'who', u'most', u'such', u'why', u'a', u'don', u'i', u'so', u'the', u'having', u'once'])\n",
        "\n",
        "(5048, 300) (4926, 300)\n"
       ]
      }
     ],
     "prompt_number": 10
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "word2vec_matrix.head()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "html": [
        "<div style=\"max-height:1000px;max-width:1500px;overflow:auto;\">\n",
        "<table border=\"1\" class=\"dataframe\">\n",
        "  <thead>\n",
        "    <tr style=\"text-align: right;\">\n",
        "      <th></th>\n",
        "      <th>0</th>\n",
        "      <th>1</th>\n",
        "      <th>2</th>\n",
        "      <th>3</th>\n",
        "      <th>4</th>\n",
        "      <th>5</th>\n",
        "      <th>6</th>\n",
        "      <th>7</th>\n",
        "      <th>8</th>\n",
        "      <th>9</th>\n",
        "      <th>...</th>\n",
        "      <th>290</th>\n",
        "      <th>291</th>\n",
        "      <th>292</th>\n",
        "      <th>293</th>\n",
        "      <th>294</th>\n",
        "      <th>295</th>\n",
        "      <th>296</th>\n",
        "      <th>297</th>\n",
        "      <th>298</th>\n",
        "      <th>299</th>\n",
        "    </tr>\n",
        "  </thead>\n",
        "  <tbody>\n",
        "    <tr>\n",
        "      <th>woods</th>\n",
        "      <td> 0.023107</td>\n",
        "      <td>-0.064096</td>\n",
        "      <td>-0.033012</td>\n",
        "      <td>-0.004874</td>\n",
        "      <td>-0.060632</td>\n",
        "      <td> 0.005319</td>\n",
        "      <td> 0.028592</td>\n",
        "      <td> 0.036677</td>\n",
        "      <td>-0.032160</td>\n",
        "      <td>-0.037889</td>\n",
        "      <td>...</td>\n",
        "      <td>-0.034348</td>\n",
        "      <td>-0.115695</td>\n",
        "      <td>-0.029499</td>\n",
        "      <td> 0.053887</td>\n",
        "      <td>-0.098718</td>\n",
        "      <td>-0.029632</td>\n",
        "      <td> 0.017293</td>\n",
        "      <td> 0.028338</td>\n",
        "      <td> 0.013004</td>\n",
        "      <td> 0.036523</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>hanging</th>\n",
        "      <td>-0.011828</td>\n",
        "      <td> 0.012929</td>\n",
        "      <td> 0.193137</td>\n",
        "      <td>-0.032013</td>\n",
        "      <td> 0.023792</td>\n",
        "      <td> 0.056206</td>\n",
        "      <td> 0.031893</td>\n",
        "      <td> 0.063223</td>\n",
        "      <td>-0.021492</td>\n",
        "      <td>-0.071596</td>\n",
        "      <td>...</td>\n",
        "      <td>-0.039783</td>\n",
        "      <td> 0.025407</td>\n",
        "      <td>-0.018741</td>\n",
        "      <td> 0.064675</td>\n",
        "      <td> 0.033112</td>\n",
        "      <td>-0.084257</td>\n",
        "      <td>-0.026043</td>\n",
        "      <td>-0.042351</td>\n",
        "      <td>-0.014099</td>\n",
        "      <td>-0.035633</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>bringing</th>\n",
        "      <td>-0.044491</td>\n",
        "      <td>-0.022517</td>\n",
        "      <td> 0.279905</td>\n",
        "      <td> 0.018783</td>\n",
        "      <td> 0.011405</td>\n",
        "      <td> 0.102786</td>\n",
        "      <td> 0.057769</td>\n",
        "      <td> 0.133539</td>\n",
        "      <td>-0.079263</td>\n",
        "      <td>-0.143306</td>\n",
        "      <td>...</td>\n",
        "      <td> 0.039194</td>\n",
        "      <td> 0.000942</td>\n",
        "      <td> 0.053785</td>\n",
        "      <td>-0.074493</td>\n",
        "      <td>-0.036535</td>\n",
        "      <td> 0.004730</td>\n",
        "      <td> 0.032593</td>\n",
        "      <td>-0.033161</td>\n",
        "      <td> 0.040146</td>\n",
        "      <td> 0.012954</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>cooking</th>\n",
        "      <td>-0.042194</td>\n",
        "      <td>-0.006758</td>\n",
        "      <td>-0.045605</td>\n",
        "      <td>-0.028938</td>\n",
        "      <td>-0.049074</td>\n",
        "      <td>-0.074931</td>\n",
        "      <td>-0.054311</td>\n",
        "      <td>-0.011877</td>\n",
        "      <td>-0.022056</td>\n",
        "      <td> 0.032138</td>\n",
        "      <td>...</td>\n",
        "      <td> 0.027557</td>\n",
        "      <td> 0.092611</td>\n",
        "      <td> 0.033355</td>\n",
        "      <td>-0.014199</td>\n",
        "      <td> 0.017343</td>\n",
        "      <td>-0.010721</td>\n",
        "      <td>-0.042198</td>\n",
        "      <td> 0.002820</td>\n",
        "      <td> 0.026314</td>\n",
        "      <td>-0.017489</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>china</th>\n",
        "      <td>-0.015148</td>\n",
        "      <td>-0.082139</td>\n",
        "      <td>-0.012488</td>\n",
        "      <td>-0.002022</td>\n",
        "      <td>-0.047960</td>\n",
        "      <td> 0.048008</td>\n",
        "      <td> 0.019317</td>\n",
        "      <td> 0.078639</td>\n",
        "      <td>-0.025027</td>\n",
        "      <td> 0.010971</td>\n",
        "      <td>...</td>\n",
        "      <td> 0.067665</td>\n",
        "      <td>-0.027157</td>\n",
        "      <td> 0.010307</td>\n",
        "      <td>-0.089299</td>\n",
        "      <td> 0.012581</td>\n",
        "      <td>-0.009840</td>\n",
        "      <td>-0.027453</td>\n",
        "      <td>-0.010525</td>\n",
        "      <td> 0.108870</td>\n",
        "      <td>-0.041462</td>\n",
        "    </tr>\n",
        "  </tbody>\n",
        "</table>\n",
        "<p>5 rows \u00d7 300 columns</p>\n",
        "</div>"
       ],
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 11,
       "text": [
        "               0         1         2         3         4         5    \\\n",
        "woods     0.023107 -0.064096 -0.033012 -0.004874 -0.060632  0.005319   \n",
        "hanging  -0.011828  0.012929  0.193137 -0.032013  0.023792  0.056206   \n",
        "bringing -0.044491 -0.022517  0.279905  0.018783  0.011405  0.102786   \n",
        "cooking  -0.042194 -0.006758 -0.045605 -0.028938 -0.049074 -0.074931   \n",
        "china    -0.015148 -0.082139 -0.012488 -0.002022 -0.047960  0.048008   \n",
        "\n",
        "               6         7         8         9      ...          290  \\\n",
        "woods     0.028592  0.036677 -0.032160 -0.037889    ...    -0.034348   \n",
        "hanging   0.031893  0.063223 -0.021492 -0.071596    ...    -0.039783   \n",
        "bringing  0.057769  0.133539 -0.079263 -0.143306    ...     0.039194   \n",
        "cooking  -0.054311 -0.011877 -0.022056  0.032138    ...     0.027557   \n",
        "china     0.019317  0.078639 -0.025027  0.010971    ...     0.067665   \n",
        "\n",
        "               291       292       293       294       295       296  \\\n",
        "woods    -0.115695 -0.029499  0.053887 -0.098718 -0.029632  0.017293   \n",
        "hanging   0.025407 -0.018741  0.064675  0.033112 -0.084257 -0.026043   \n",
        "bringing  0.000942  0.053785 -0.074493 -0.036535  0.004730  0.032593   \n",
        "cooking   0.092611  0.033355 -0.014199  0.017343 -0.010721 -0.042198   \n",
        "china    -0.027157  0.010307 -0.089299  0.012581 -0.009840 -0.027453   \n",
        "\n",
        "               297       298       299  \n",
        "woods     0.028338  0.013004  0.036523  \n",
        "hanging  -0.042351 -0.014099 -0.035633  \n",
        "bringing -0.033161  0.040146  0.012954  \n",
        "cooking   0.002820  0.026314 -0.017489  \n",
        "china    -0.010525  0.108870 -0.041462  \n",
        "\n",
        "[5 rows x 300 columns]"
       ]
      }
     ],
     "prompt_number": 11
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# hand off to r\n",
      "word2vec_matrix.to_csv('word2vec_matrix.csv')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 12
    }
   ],
   "metadata": {}
  }
 ]
}