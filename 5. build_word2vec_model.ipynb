{
 "metadata": {
  "name": "",
  "signature": "sha256:8fe6928892ecfc4ab2a0756085eaede473a4220618ca663d84bd1214ccec7044"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import pandas as pd\n",
      "from pandas import DataFrame\n",
      "import re\n",
      "import nltk\n",
      "import logging\n",
      "from gensim.models import word2vec\n",
      "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 1
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "nyt_npr_gr = pd.read_csv('intermediary/nyt_npr_gr.csv')\n",
      "desc = nyt_npr_gr['desc']"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 2
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# clean descriptions and parse them into sentences made up of word lists\n",
      "def sent_to_words(sent):\n",
      "    sent = re.sub(r'[^a-z]', ' ', sent.lower())\n",
      "    words = sent.split()\n",
      "    return words\n",
      "\n",
      "sent_tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
      "def desc_to_sents(desc):\n",
      "    desc = desc.decode('utf-8')\n",
      "    sents = sent_tokenizer.tokenize(desc.strip())\n",
      "    sents = [sent_to_words(sent) for sent in sents]\n",
      "    return sents\n",
      "\n",
      "desc = [desc_to_sents(d) for d in desc]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 3
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "desc = sum(desc, [])\n",
      "len(desc)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 4,
       "text": [
        "33158"
       ]
      }
     ],
     "prompt_number": 4
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# train a word vector model\n",
      "word2vec_model = word2vec.Word2Vec(desc, workers = 4, min_count = 10, size = 300, window = 50, sample = .001, seed = 2015)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 5
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# test the model with a couple examples\n",
      "word2vec_model.most_similar('harry')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 6,
       "text": [
        "[(u'potter', 0.8953883647918701),\n",
        " (u'sorcerer', 0.8337182998657227),\n",
        " (u'boxed', 0.8081557154655457),\n",
        " (u'box', 0.7767009735107422),\n",
        " (u'delivered', 0.7428127527236938),\n",
        " (u'hogwarts', 0.7119302749633789),\n",
        " (u'prisoner', 0.6939250230789185),\n",
        " (u'geek', 0.6736477017402649),\n",
        " (u'stone', 0.6646313667297363),\n",
        " (u'hardcover', 0.6436514258384705)]"
       ]
      }
     ],
     "prompt_number": 6
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "word2vec_model.most_similar(positive = ['king', 'woman'], negative = ['man'])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 7,
       "text": [
        "[(u'throne', 0.6862560510635376),\n",
        " (u'bound', 0.6386792659759521),\n",
        " (u'lost', 0.6132731437683105),\n",
        " (u'kingdom', 0.604865550994873),\n",
        " (u'england', 0.6025775671005249),\n",
        " (u'desert', 0.6006991863250732),\n",
        " (u'empire', 0.5962868928909302),\n",
        " (u'fighting', 0.5865373015403748),\n",
        " (u'clan', 0.5846682786941528),\n",
        " (u'enemy', 0.5844212770462036)]"
       ]
      }
     ],
     "prompt_number": 7
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# save the model\n",
      "word2vec_model.init_sims(replace = True)\n",
      "word2vec_model.save('intermediary/word2vec_model')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 8
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# combine the word vector matrix with the chosen vocabulary\n",
      "word2vec_matrix = DataFrame(word2vec_model.syn0, index = word2vec_model.index2word)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 9
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# remove stop words from the matrix\n",
      "stop_words = nltk.corpus.stopwords.words('english')\n",
      "stop_words = set(stop_words) & set(word2vec_model.index2word)\n",
      "print stop_words\n",
      "\n",
      "word2vec_matrix = word2vec_matrix.drop(stop_words)\n",
      "print '\\n', word2vec_model.syn0.shape, word2vec_matrix.shape"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "set([u'all', u'just', u'being', u'over', u'both', u'through', u'its', u'before', u'with', u'had', u'should', u'to', u'only', u'under', u'has', u'do', u'them', u'his', u'very', u'they', u'not', u'during', u'now', u'him', u'nor', u'did', u'this', u't', u'each', u'where', u'because', u'doing', u'some', u'are', u'our', u'ourselves', u'out', u'what', u'for', u'below', u'does', u'above', u'between', u'she', u'be', u'we', u'after', u'here', u'hers', u'by', u'on', u'about', u'of', u'against', u's', u'or', u'own', u'into', u'yourself', u'down', u'your', u'from', u'her', u'whom', u'there', u'been', u'few', u'too', u'themselves', u'was', u'until', u'more', u'himself', u'that', u'but', u'off', u'herself', u'than', u'those', u'he', u'me', u'myself', u'these', u'up', u'will', u'while', u'can', u'were', u'my', u'and', u'then', u'is', u'in', u'am', u'it', u'an', u'as', u'itself', u'at', u'have', u'further', u'their', u'if', u'again', u'no', u'when', u'same', u'any', u'how', u'other', u'which', u'you', u'who', u'most', u'such', u'why', u'a', u'don', u'i', u'having', u'so', u'the', u'yours', u'once'])\n",
        "\n",
        "(6938, 300) (6814, 300)\n"
       ]
      }
     ],
     "prompt_number": 10
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "word2vec_matrix.head()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "html": [
        "<div style=\"max-height:1000px;max-width:1500px;overflow:auto;\">\n",
        "<table border=\"1\" class=\"dataframe\">\n",
        "  <thead>\n",
        "    <tr style=\"text-align: right;\">\n",
        "      <th></th>\n",
        "      <th>0</th>\n",
        "      <th>1</th>\n",
        "      <th>2</th>\n",
        "      <th>3</th>\n",
        "      <th>4</th>\n",
        "      <th>5</th>\n",
        "      <th>6</th>\n",
        "      <th>7</th>\n",
        "      <th>8</th>\n",
        "      <th>9</th>\n",
        "      <th>...</th>\n",
        "      <th>290</th>\n",
        "      <th>291</th>\n",
        "      <th>292</th>\n",
        "      <th>293</th>\n",
        "      <th>294</th>\n",
        "      <th>295</th>\n",
        "      <th>296</th>\n",
        "      <th>297</th>\n",
        "      <th>298</th>\n",
        "      <th>299</th>\n",
        "    </tr>\n",
        "  </thead>\n",
        "  <tbody>\n",
        "    <tr>\n",
        "      <th>woods</th>\n",
        "      <td>-0.037903</td>\n",
        "      <td> 0.043488</td>\n",
        "      <td>-0.032664</td>\n",
        "      <td> 0.021220</td>\n",
        "      <td>-0.041929</td>\n",
        "      <td>-0.077800</td>\n",
        "      <td>-0.005916</td>\n",
        "      <td>-0.020100</td>\n",
        "      <td> 0.031343</td>\n",
        "      <td>-0.027321</td>\n",
        "      <td>...</td>\n",
        "      <td>-0.085690</td>\n",
        "      <td> 0.065105</td>\n",
        "      <td>-0.136628</td>\n",
        "      <td>-0.014861</td>\n",
        "      <td>-0.007346</td>\n",
        "      <td>-0.125117</td>\n",
        "      <td>-0.055288</td>\n",
        "      <td> 0.003328</td>\n",
        "      <td> 0.050011</td>\n",
        "      <td>-0.050581</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>hanging</th>\n",
        "      <td>-0.023658</td>\n",
        "      <td>-0.010922</td>\n",
        "      <td> 0.019864</td>\n",
        "      <td>-0.000075</td>\n",
        "      <td>-0.029777</td>\n",
        "      <td>-0.071766</td>\n",
        "      <td> 0.023761</td>\n",
        "      <td> 0.062506</td>\n",
        "      <td>-0.004656</td>\n",
        "      <td>-0.037634</td>\n",
        "      <td>...</td>\n",
        "      <td>-0.041356</td>\n",
        "      <td>-0.037855</td>\n",
        "      <td> 0.029563</td>\n",
        "      <td> 0.195065</td>\n",
        "      <td> 0.103817</td>\n",
        "      <td> 0.032035</td>\n",
        "      <td>-0.049153</td>\n",
        "      <td>-0.034522</td>\n",
        "      <td> 0.082199</td>\n",
        "      <td> 0.029915</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>originality</th>\n",
        "      <td> 0.086211</td>\n",
        "      <td> 0.057843</td>\n",
        "      <td> 0.019934</td>\n",
        "      <td>-0.069313</td>\n",
        "      <td>-0.043247</td>\n",
        "      <td>-0.020188</td>\n",
        "      <td> 0.052656</td>\n",
        "      <td>-0.050637</td>\n",
        "      <td>-0.019730</td>\n",
        "      <td>-0.130294</td>\n",
        "      <td>...</td>\n",
        "      <td>-0.032008</td>\n",
        "      <td> 0.078367</td>\n",
        "      <td> 0.072018</td>\n",
        "      <td>-0.048366</td>\n",
        "      <td>-0.036309</td>\n",
        "      <td>-0.075225</td>\n",
        "      <td> 0.115748</td>\n",
        "      <td>-0.005478</td>\n",
        "      <td>-0.055577</td>\n",
        "      <td>-0.136861</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>bringing</th>\n",
        "      <td>-0.059434</td>\n",
        "      <td> 0.007546</td>\n",
        "      <td>-0.031168</td>\n",
        "      <td>-0.042804</td>\n",
        "      <td>-0.002806</td>\n",
        "      <td>-0.037217</td>\n",
        "      <td> 0.026832</td>\n",
        "      <td> 0.044287</td>\n",
        "      <td>-0.076094</td>\n",
        "      <td>-0.020785</td>\n",
        "      <td>...</td>\n",
        "      <td>-0.064985</td>\n",
        "      <td> 0.077979</td>\n",
        "      <td>-0.011947</td>\n",
        "      <td> 0.090913</td>\n",
        "      <td> 0.051448</td>\n",
        "      <td>-0.114672</td>\n",
        "      <td>-0.009398</td>\n",
        "      <td>-0.055403</td>\n",
        "      <td> 0.029976</td>\n",
        "      <td>-0.108670</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>cooking</th>\n",
        "      <td>-0.018163</td>\n",
        "      <td> 0.005666</td>\n",
        "      <td>-0.047516</td>\n",
        "      <td>-0.001223</td>\n",
        "      <td>-0.029145</td>\n",
        "      <td>-0.011539</td>\n",
        "      <td>-0.010879</td>\n",
        "      <td>-0.073916</td>\n",
        "      <td> 0.017844</td>\n",
        "      <td> 0.017010</td>\n",
        "      <td>...</td>\n",
        "      <td> 0.032261</td>\n",
        "      <td> 0.090811</td>\n",
        "      <td>-0.067375</td>\n",
        "      <td> 0.018544</td>\n",
        "      <td>-0.041720</td>\n",
        "      <td>-0.064842</td>\n",
        "      <td> 0.027980</td>\n",
        "      <td>-0.025852</td>\n",
        "      <td>-0.016011</td>\n",
        "      <td>-0.047859</td>\n",
        "    </tr>\n",
        "  </tbody>\n",
        "</table>\n",
        "<p>5 rows \u00d7 300 columns</p>\n",
        "</div>"
       ],
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 11,
       "text": [
        "                  0         1         2         3         4         5    \\\n",
        "woods       -0.037903  0.043488 -0.032664  0.021220 -0.041929 -0.077800   \n",
        "hanging     -0.023658 -0.010922  0.019864 -0.000075 -0.029777 -0.071766   \n",
        "originality  0.086211  0.057843  0.019934 -0.069313 -0.043247 -0.020188   \n",
        "bringing    -0.059434  0.007546 -0.031168 -0.042804 -0.002806 -0.037217   \n",
        "cooking     -0.018163  0.005666 -0.047516 -0.001223 -0.029145 -0.011539   \n",
        "\n",
        "                  6         7         8         9      ...          290  \\\n",
        "woods       -0.005916 -0.020100  0.031343 -0.027321    ...    -0.085690   \n",
        "hanging      0.023761  0.062506 -0.004656 -0.037634    ...    -0.041356   \n",
        "originality  0.052656 -0.050637 -0.019730 -0.130294    ...    -0.032008   \n",
        "bringing     0.026832  0.044287 -0.076094 -0.020785    ...    -0.064985   \n",
        "cooking     -0.010879 -0.073916  0.017844  0.017010    ...     0.032261   \n",
        "\n",
        "                  291       292       293       294       295       296  \\\n",
        "woods        0.065105 -0.136628 -0.014861 -0.007346 -0.125117 -0.055288   \n",
        "hanging     -0.037855  0.029563  0.195065  0.103817  0.032035 -0.049153   \n",
        "originality  0.078367  0.072018 -0.048366 -0.036309 -0.075225  0.115748   \n",
        "bringing     0.077979 -0.011947  0.090913  0.051448 -0.114672 -0.009398   \n",
        "cooking      0.090811 -0.067375  0.018544 -0.041720 -0.064842  0.027980   \n",
        "\n",
        "                  297       298       299  \n",
        "woods        0.003328  0.050011 -0.050581  \n",
        "hanging     -0.034522  0.082199  0.029915  \n",
        "originality -0.005478 -0.055577 -0.136861  \n",
        "bringing    -0.055403  0.029976 -0.108670  \n",
        "cooking     -0.025852 -0.016011 -0.047859  \n",
        "\n",
        "[5 rows x 300 columns]"
       ]
      }
     ],
     "prompt_number": 11
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# hand off to r\n",
      "word2vec_matrix.to_csv('intermediary/word2vec_matrix.csv')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 12
    }
   ],
   "metadata": {}
  }
 ]
}